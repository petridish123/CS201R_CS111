{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Did some cleaning here.\n",
    "- Removed weird parenthesized numbers after lab and homework names, as I didn't like them.\n",
    "- Added columns for each exam calculating letter grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Lab 02', 'Lab 03', 'Lab 04', 'Lab 05', 'Lab 06', 'Lab 07', 'Lab 08',\n",
      "       'Lab 09', 'Lab 10', 'Lab 11', 'Lab 12', 'Lab 13', 'Lab 14', 'Lab 15',\n",
      "       'Lab 16', 'Lab 17', 'Lab 20', 'Lab 21', 'Lab 22', 'Lab 23',\n",
      "       'Homework 1', 'Homework 2', 'Homework 3', 'Homework 4', 'Homework 5',\n",
      "       'Homework 6', 'Homework 7', 'Project 1', 'Project 2A', 'Project 2B',\n",
      "       'Project 3', 'Project 4', 'Midterm 1', 'Midterm 2', 'Final Exam',\n",
      "       'Midterm 1 Grade', 'Midterm 2 Grade', 'Final Exam Grade'],\n",
      "      dtype='object')\n",
      "     Final Exam Final Exam Grade\n",
      "0          64.0               A+\n",
      "1          54.0                B\n",
      "2          56.0               B+\n",
      "3           0.0                E\n",
      "4          56.0               B+\n",
      "..          ...              ...\n",
      "401        54.0                B\n",
      "402        61.0                A\n",
      "403        47.5                C\n",
      "404        58.0               A-\n",
      "405        57.0               B+\n",
      "\n",
      "[406 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Training_w_midterm.csv')\n",
    "\n",
    "data.rename(columns=lambda x: re.sub('\\s\\([^)]*\\)', '', x), inplace=True)\n",
    "\n",
    "def score_to_grade(score, mult):\n",
    "    score *= mult\n",
    "    if score >= 97:\n",
    "        return 'A+'\n",
    "    elif score >= 94:\n",
    "        return 'A'\n",
    "    elif score >= 90:\n",
    "        return 'A-'\n",
    "    elif score >= 87:\n",
    "        return 'B+'\n",
    "    elif score >= 84:\n",
    "        return 'B'\n",
    "    elif score >= 80:\n",
    "        return 'B-'\n",
    "    elif score >= 77:\n",
    "        return 'C+'\n",
    "    elif score >= 74:\n",
    "        return 'C'\n",
    "    elif score >= 70:\n",
    "        return 'C-'\n",
    "    elif score >= 67:\n",
    "        return 'D+'\n",
    "    elif score >= 64:\n",
    "        return 'D'\n",
    "    elif score >= 60:\n",
    "        return 'D-'\n",
    "    else:\n",
    "        return 'E'\n",
    "\n",
    "data['Midterm 1 Grade'] = data['Midterm 1'].apply(lambda score: score_to_grade(score, 2.5))\n",
    "data['Midterm 2 Grade'] = data['Midterm 2'].apply(lambda score: score_to_grade(score, 2.5))\n",
    "data['Final Exam Grade'] = data['Final Exam'].apply(lambda score: score_to_grade(score, 1.5625))\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of students in the class: 406\n",
      "Number of columns: 33\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of students in the class: {len(data)}')\n",
    "print(f'Number of columns: {len(data.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes out the pre-Midterm 1 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Homework 1', 'Homework 2', 'Homework 3', 'Lab 02', 'Lab 03', 'Lab 04',\n",
      "       'Lab 05', 'Lab 06', 'Lab 07', 'Lab 08', 'Lab 09', 'Lab 10', 'Project 1',\n",
      "       'Project 2A', 'Midterm 1', 'Midterm 1 Grade'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "premt1_data = pd.DataFrame()\n",
    "\n",
    "homework_premt1 = data.filter(regex='^Homework')\n",
    "\n",
    "for column in homework_premt1.columns:\n",
    "    _, val = column.split()\n",
    "    if int(val) > 3:\n",
    "        homework_premt1.drop([column], axis=1, inplace=True)\n",
    "\n",
    "premt1_data[homework_premt1.columns] = homework_premt1\n",
    "\n",
    "lab_premt1 = data.filter(regex='^Lab')\n",
    "\n",
    "for column in lab_premt1.columns:\n",
    "    _, val = column.split()\n",
    "    if int(val) > 10:\n",
    "        lab_premt1.drop([column], axis=1, inplace=True)\n",
    "\n",
    "premt1_data[lab_premt1.columns] = lab_premt1\n",
    "\n",
    "premt1_data[['Project 1', 'Project 2A']] = data[['Project 1', 'Project 2A']]\n",
    "\n",
    "premt1_data[['Midterm 1', 'Midterm 1 Grade']] = data[['Midterm 1', 'Midterm 1 Grade']]\n",
    "\n",
    "print(premt1_data.columns)\n",
    "\n",
    "premt1_data.to_csv('pre_mt1_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Homework 1', 'Homework 2', 'Homework 3', 'Homework 4', 'Homework 5',\n",
      "       'Homework 6', 'Lab 02', 'Lab 03', 'Lab 04', 'Lab 05', 'Lab 06',\n",
      "       'Lab 07', 'Lab 08', 'Lab 09', 'Lab 10', 'Lab 11', 'Lab 12', 'Lab 13',\n",
      "       'Lab 14', 'Lab 15', 'Lab 16', 'Lab 17', 'Project 1', 'Project 2A',\n",
      "       'Project 2B', 'Project 3', 'Midterm 1', 'Midterm 2', 'Midterm 2 Grade'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "premt2_data = pd.DataFrame()\n",
    "\n",
    "homework_premt2 = data.filter(regex='^Homework')\n",
    "\n",
    "for column in homework_premt2.columns:\n",
    "    _, val = column.split()\n",
    "    if int(val) > 6:\n",
    "        homework_premt2.drop([column], axis=1, inplace=True)\n",
    "\n",
    "premt2_data[homework_premt2.columns] = homework_premt2\n",
    "\n",
    "lab_premt2 = data.filter(regex='^Lab')\n",
    "\n",
    "for column in lab_premt2.columns:\n",
    "    _, val = column.split()\n",
    "    if int(val) > 17:\n",
    "        lab_premt2.drop([column], axis=1, inplace=True)\n",
    "\n",
    "premt2_data[lab_premt2.columns] = lab_premt2\n",
    "\n",
    "premt2_data[['Project 1', 'Project 2A', 'Project 2B', 'Project 3']] = data[['Project 1', 'Project 2A', 'Project 2B', 'Project 3']]\n",
    "\n",
    "premt2_data['Midterm 1'] = data['Midterm 1']\n",
    "\n",
    "premt2_data[['Midterm 2', 'Midterm 2 Grade']] = data[['Midterm 2', 'Midterm 2 Grade']]\n",
    "\n",
    "print(premt2_data.columns)\n",
    "\n",
    "premt2_data.to_csv('pre_mt2_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Lab 02', 'Lab 03', 'Lab 04', 'Lab 05', 'Lab 06', 'Lab 07', 'Lab 08',\n",
      "       'Lab 09', 'Lab 10', 'Lab 11', 'Lab 12', 'Lab 13', 'Lab 14', 'Lab 15',\n",
      "       'Lab 16', 'Lab 17', 'Lab 20', 'Lab 21', 'Lab 22', 'Lab 23',\n",
      "       'Homework 1', 'Homework 2', 'Homework 3', 'Homework 4', 'Homework 5',\n",
      "       'Homework 6', 'Homework 7', 'Project 1', 'Project 2A', 'Project 2B',\n",
      "       'Project 3', 'Project 4', 'Midterm 1', 'Midterm 2', 'Final Exam',\n",
      "       'Final Exam Grade'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "prefinal_data = data.drop(['Midterm 1 Grade', 'Midterm 2 Grade'], axis=1)\n",
    "\n",
    "print(prefinal_data.columns)\n",
    "\n",
    "prefinal_data.to_csv('pre_final_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Here, I put together some input data for my algorithm. The goal of this algorithm is to model classification using all homework, lab, and project data to predict the midterm letter grade (A+ to E)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Lab 02', 'Lab 03', 'Lab 04', 'Lab 05', 'Lab 06', 'Lab 07', 'Lab 08',\n",
      "       'Lab 09', 'Lab 10', 'Lab 11', 'Lab 12', 'Lab 13', 'Lab 14', 'Lab 15',\n",
      "       'Lab 16', 'Lab 17', 'Lab 20', 'Lab 21', 'Lab 22', 'Lab 23',\n",
      "       'Homework 1', 'Homework 2', 'Homework 3', 'Homework 4', 'Homework 5',\n",
      "       'Homework 6', 'Homework 7', 'Project 1', 'Project 2A', 'Project 2B',\n",
      "       'Project 3', 'Project 4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "input_data = data.filter(regex='^(Homework|Lab|Project).*')\n",
    "print(input_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Midterm 1', 'Midterm 2', 'Final Exam', 'Final Score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "label_data = data[['Midterm 1', 'Midterm 2', 'Final Exam', 'Final Score']]\n",
    "print(label_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where the juicy stuff happens. I used a random forest regressor to model this data, as I've never used it before and kinda wanted to try it. I also think it somewhat fits our data. I divided the data with a train-test split of 80/20. I did a grid search over the parameters in `param_grid` to get the best set of parameters for our model. You can see the calculated best parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = input_data.to_numpy()\n",
    "y = label_data.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "random_forest_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_random_forest_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it did pretty well. The values for R^2 and MSE aren't bad at all, and the sample prediction, taken from the test set, is pretty dang close. It would make more sense to divide predictions for Midterm 1, Midterm 2, and the Final grades into separate models, as those things are temporally dependent on each other. However, even without that consideration, this model did really well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6137633299246896\n",
      "Mean squared error: 30.016585756739374\n",
      "Student 1 predicted values (Midterm 1, Midterm 2, Final Exam, Final Score): [[36.61293053 43.24134006 53.95283325 97.24878178]]\n",
      "Student 1 actual values (Midterm 1, Midterm 2, Final Exam, Final Score): [37.0 45.0 61.0 '99.81']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = best_random_forest_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "\n",
    "print(f\"Student 1 predicted values (Midterm 1, Midterm 2, Final Exam, Final Score): {best_random_forest_model.predict(X_test[0,:].reshape(1, -1))}\")\n",
    "print(f\"Student 1 actual values (Midterm 1, Midterm 2, Final Exam, Final Score): {y_test[0,:]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
