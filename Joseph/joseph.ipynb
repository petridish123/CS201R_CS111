{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Did some cleaning here.\n",
    "- Removed weird parenthesized numbers after lab and homework names, as I didn't like them.\n",
    "- Removed rows with \"(read only)\" in them (I think it was only one).\n",
    "- Replaced grades with enumerated values from the `grade_mapping` dictionary.\n",
    "- Filled empty values with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Lab 02', 'Lab 03', 'Lab 04', 'Lab 05', 'Lab 06', 'Lab 07', 'Lab 08',\n",
      "       'Lab 09', 'Lab 10', 'Lab 11', 'Lab 12', 'Lab 13', 'Lab 14', 'Lab 15',\n",
      "       'Lab 16', 'Lab 17', 'Lab 20', 'Lab 21', 'Lab 22', 'Lab 23',\n",
      "       'Homework 1', 'Homework 2', 'Homework 3', 'Homework 4', 'Homework 5',\n",
      "       'Homework 6', 'Homework 7', 'Project 1', 'Project 2A', 'Project 2B',\n",
      "       'Project 3', 'Project 4', 'Final Exam'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Training_set.csv')\n",
    "\n",
    "data.rename(columns=lambda x: re.sub('\\s\\([^)]*\\)', '', x), inplace=True)\n",
    "\n",
    "# data = data[~data.isin(['(read only)']).any(axis=1)]\n",
    "\n",
    "# grade_mapping = {'A': 0, 'A-': 1, 'B+': 2, 'B': 3, 'B-': 4,\n",
    "#                  'C+': 5, 'C': 6, 'C-': 7, 'D+': 8, 'D': 9,\n",
    "#                  'D-': 10, 'E': 11}\n",
    "\n",
    "# for column in data.columns:\n",
    "#     if column.endswith('Grade'):\n",
    "#         data[column] = data[column].map(grade_mapping)\n",
    "\n",
    "data.fillna(0, inplace=True)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of students in the class: 406\n",
      "Number of columns: 33\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of students in the class: {len(data)}')\n",
    "print(f'Number of columns: {len(data.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is only data about labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Lab 02', 'Lab 03', 'Lab 04', 'Lab 05', 'Lab 06', 'Lab 07', 'Lab 08',\n",
      "       'Lab 09', 'Lab 10', 'Lab 11', 'Lab 12', 'Lab 13', 'Lab 14', 'Lab 15',\n",
      "       'Lab 16', 'Lab 17', 'Lab 20', 'Lab 21', 'Lab 22', 'Lab 23'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "lab_data = data.filter(regex='^Lab')\n",
    "print(lab_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is only data about homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Homework 1', 'Homework 2', 'Homework 3', 'Homework 4', 'Homework 5',\n",
      "       'Homework 6', 'Homework 7'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "hw_data = data.filter(regex='^Homework')\n",
    "print(hw_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is only data about projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Project 1', 'Project 2A', 'Project 2B', 'Project 3', 'Project 4'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "proj_data = data.filter(regex='^Project')\n",
    "print(proj_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Here, I put together some input data for my algorithm. The goal of this algorithm is to model classification comparing all homework, lab, and project data to the grades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Lab 02', 'Lab 03', 'Lab 04', 'Lab 05', 'Lab 06', 'Lab 07', 'Lab 08',\n",
      "       'Lab 09', 'Lab 10', 'Lab 11', 'Lab 12', 'Lab 13', 'Lab 14', 'Lab 15',\n",
      "       'Lab 16', 'Lab 17', 'Lab 20', 'Lab 21', 'Lab 22', 'Lab 23',\n",
      "       'Homework 1', 'Homework 2', 'Homework 3', 'Homework 4', 'Homework 5',\n",
      "       'Homework 6', 'Homework 7', 'Project 1', 'Project 2A', 'Project 2B',\n",
      "       'Project 3', 'Project 4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "input_data = data.filter(regex='^(Homework|Lab|Project).*')\n",
    "print(input_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Midterm 1', 'Midterm 2', 'Final Exam', 'Final Score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "label_data = data[['Midterm 1', 'Midterm 2', 'Final Exam', 'Final Score']]\n",
    "print(label_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where the juicy stuff happens. I used a random forest regressor to model this data, as I've never used it before and kinda wanted to try it. I also think it somewhat fits our data. I divided the data with a train-test split of 80/20. I did a grid search over the parameters in `param_grid` to get the best set of parameters for our model. You can see the calculated best parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = input_data.to_numpy()\n",
    "y = label_data.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "random_forest_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_random_forest_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it did pretty well. The values for R^2 and MSE aren't bad at all, and the sample prediction, taken from the test set, is pretty dang close. It would make more sense to divide predictions for Midterm 1, Midterm 2, and the Final grades into separate models, as those things are temporally dependent on each other. However, even without that consideration, this model did really well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6137633299246896\n",
      "Mean squared error: 30.016585756739374\n",
      "Student 1 predicted values (Midterm 1, Midterm 2, Final Exam, Final Score): [[36.61293053 43.24134006 53.95283325 97.24878178]]\n",
      "Student 1 actual values (Midterm 1, Midterm 2, Final Exam, Final Score): [37.0 45.0 61.0 '99.81']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = best_random_forest_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "\n",
    "print(f\"Student 1 predicted values (Midterm 1, Midterm 2, Final Exam, Final Score): {best_random_forest_model.predict(X_test[0,:].reshape(1, -1))}\")\n",
    "print(f\"Student 1 actual values (Midterm 1, Midterm 2, Final Exam, Final Score): {y_test[0,:]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
