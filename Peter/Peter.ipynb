{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peter Gordon's section of the code\n",
    "## tasks:\n",
    "- manipulate the data\n",
    "- use two or three different models with hyperparameters\n",
    "\n",
    "## outcome:\n",
    "- to find what manipulation of the data will predict the final grade the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_201704/2232864728.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#managing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "grade_base_data = pd.read_csv(\"pre_final_train.csv\")\n",
    "\n",
    "#setting the X, y, train and test\n",
    "X = grade_base_data.iloc[:,:-1]\n",
    "y = grade_base_data.iloc[:,-1]\n",
    "\n",
    "#setting the train test split to 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peak at the data.\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At this point I have looked at the data\n",
    "* The data is split up so that X is all data except the Final grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.5644413388680896\n",
      "Testing score: 0.37337791105658735\n"
     ]
    }
   ],
   "source": [
    "#Setting up the Classifier clf as a KNN regressor with default parameters\n",
    "clf = KNeighborsRegressor()\n",
    "#Fitting the classifier with the training data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#checking the score of the default KNN Regressor\n",
    "print(f\"Training score: {clf.score(X_train,y_train)}\")\n",
    "print(f\"Testing score: {clf.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.8801448079370845\n",
      "Testing score: 0.3512019891192997\n"
     ]
    }
   ],
   "source": [
    "#Setting up the Classifier clf as a KNN regressor Changing the parameters\n",
    "clf = KNeighborsRegressor(n_neighbors= 5, weights=\"distance\", algorithm=\"auto\")\n",
    "#Fitting the classifier with the training data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#checking the score of the parameter KNN Regressor\n",
    "print(f\"Training score: {clf.score(X_train,y_train)}\")\n",
    "print(f\"Testing score: {clf.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'B-'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# fitting the data\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#checking the score of the default DT Regressor\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf\u001b[38;5;241m.\u001b[39mscore(X_train,y_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:1377\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \n\u001b[1;32m   1351\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:318\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m DOUBLE \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m y\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous:\n\u001b[0;32m--> 318\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDOUBLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m max_depth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'B-'"
     ]
    }
   ],
   "source": [
    "# Setting up clf as a decision tree regressor default parameters\n",
    "_base_data = pd.read_csv(\"pre_final_train.csv\")\n",
    "\n",
    "#setting the X, y, train and test\n",
    "X = grade_base_data.iloc[:,:-1]\n",
    "y = grade_base_data.iloc[:,-1]\n",
    "\n",
    "#setting the train test split to 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)\n",
    "clf = DecisionTreeRegressor(max_depth=6)\n",
    "# fitting the data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#checking the score of the default DT Regressor\n",
    "print(f\"Training score: {clf.score(X_train,y_train)}\")\n",
    "print(f\"Testing score: {clf.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome of changing the parameters was:\n",
    "* Found that the best testing accuracy was never greater than 37% with knn regressor\n",
    "* Training accuracy clearly overfit a little\n",
    "* KNN may not be the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begining of data manipulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GASLIGHT GATEKEEP GIRLBOSS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging all the lab grades\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"pre_final_train.csv\")\n",
    "def manipulate(data):\n",
    "    averages = []\n",
    "    for index, row in data.iterrows():\n",
    "        total = 0\n",
    "        n = 20\n",
    "        for column in data:\n",
    "            if \"Lab\" in column:\n",
    "                total += row[column]\n",
    "        average = total/n\n",
    "        averages.append(average)\n",
    "    data[\"avg_lab\"] = averages\n",
    "\n",
    "\n",
    "\n",
    "    #averaging the homeworks\n",
    "    averages = []\n",
    "    for index, row in data.iterrows():\n",
    "        total = 0\n",
    "        n = 7\n",
    "        for column in data:\n",
    "            if \"Homework\" in column:\n",
    "                total += row[column]\n",
    "        average = total/n\n",
    "        averages.append(average)\n",
    "    data[\"avg_hw\"] = averages\n",
    "\n",
    "\n",
    "    averages = []\n",
    "    for index, row in data.iterrows():\n",
    "        total = 0\n",
    "        n = 5\n",
    "        for column in data:\n",
    "            if \"Proj\" in column:\n",
    "                total += row[column]\n",
    "                \n",
    "        average = total/n\n",
    "        averages.append(average)\n",
    "        \n",
    "\n",
    "    data[\"avg_proj\"] = averages\n",
    "    return data\n",
    "\n",
    "data = manipulate(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9481675648069456\n",
      "Testing score: 0.3149486887145838\n"
     ]
    }
   ],
   "source": [
    "# Setting up clf as a decision tree regressor default parameters\n",
    "_base_data = pd.read_csv(\"pre_final_train.csv\")\n",
    "\n",
    "#setting the X, y, train and test\n",
    "X = grade_base_data.iloc[:,:-1]\n",
    "y = grade_base_data.iloc[:,-1]\n",
    "\n",
    "\n",
    "reg_y = data[\"Final Exam\"]\n",
    "reg_x = data[[\"avg_lab\",\"avg_hw\",\"avg_proj\", \"Midterm 1\", \"Midterm 2\"]]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reg_x, reg_y, test_size=.2)\n",
    "\n",
    "\n",
    "#setting the train test split to 80/20\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)\n",
    "clf = DecisionTreeRegressor(max_depth=10)\n",
    "# fitting the data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#checking the score of the default DT Regressor\n",
    "print(f\"Training score: {clf.score(X_train,y_train)}\")\n",
    "print(f\"Testing score: {clf.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9460282217149384\n",
      "Testing score: 0.5731317953552163\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeRegressor(max_depth=10,splitter=\"best\", criterion=\"squared_error\")\n",
    "# fitting the data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#checking the score of the default DT Regressor\n",
    "print(f\"Training score: {clf.score(X_train,y_train)}\")\n",
    "print(f\"Testing score: {clf.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.6673569527673019\n",
      "Testing score: 0.6063892445947866\n",
      "Training score: 0.9839252700462909\n",
      "Testing score: 0.70167917361749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = manipulate(pd.read_csv(\"pre_final_train.csv\"))\n",
    "test = manipulate(pd.read_csv(\"pre_final_test.csv\"))\n",
    "\n",
    "\n",
    "reg_y = data[\"Final Exam\"]\n",
    "reg_x = data[[\"avg_lab\",\"avg_hw\",\"avg_proj\", \"Midterm 1\", \"Midterm 2\"]]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reg_x, reg_y, test_size=.2)\n",
    "\n",
    "clf = KNeighborsRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training score: {clf.score(X_train,y_train)}\")\n",
    "print(f\"Testing score: {clf.score(X_test,y_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "clf = KNeighborsRegressor(n_neighbors= 7, weights=\"distance\", algorithm=\"auto\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training score: {clf.score(X_train,y_train)}\")\n",
    "print(f\"Testing score: {clf.score(X_test,y_test)}\")\n",
    "# class_y = data[\"Final Exam Grade\"]\n",
    "# class_x = data[[\"avg_lab\",\"avg_hw\",\"avg_proj\", \"Midterm 1\", \"Midterm 2\"]]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(class_x, class_y, test_size=.2)\n",
    "\n",
    "# clf = KNeighborsClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# print(f\"Training score: {clf.score(X_train,y_train)}\")\n",
    "# print(f\"Testing score: {clf.score(X_test,y_test)}\")\n",
    "\n",
    "# test_reg_data = pd.read_csv(\"pre_final_test.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test score: 0.7314454690442004\n",
      "DT test score 0.4586872312658392\n"
     ]
    }
   ],
   "source": [
    "data = manipulate(pd.read_csv(\"pre_final_train.csv\"))\n",
    "test = manipulate(pd.read_csv(\"pre_final_test.csv\"))\n",
    "\n",
    "reg_y = data[\"Final Exam\"]\n",
    "reg_x = data[[\"avg_lab\",\"avg_hw\",\"avg_proj\", \"Midterm 1\", \"Midterm 2\"]]\n",
    "test_y = test[\"Final Exam\"]\n",
    "test_x = test[[\"avg_lab\",\"avg_hw\",\"avg_proj\", \"Midterm 1\", \"Midterm 2\"]]\n",
    "\n",
    "\n",
    "clf2 = DecisionTreeRegressor(max_depth=10)\n",
    "clf2.fit(reg_x,reg_y)\n",
    "clf = KNeighborsRegressor(n_neighbors= 5, weights=\"distance\", algorithm=\"auto\")\n",
    "clf.fit(reg_x,reg_y)\n",
    "\n",
    "print(f\"KNN Test score: {clf.score(test_x,test_y)}\")\n",
    "print(f\"DT test score {clf2.score(test_x,test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55.3142596  52.26836243 50.64561728 51.62681269 48.64764764 58.5\n",
      " 53.37343703 57.5        48.85561723 58.25       55.43199775 54.36635936\n",
      " 50.50793672 49.40446706  5.16163699 54.57142857 54.57142857 51.19152188\n",
      " 54.11573954 52.16051721 57.05434139 59.42857143 60.         50.14319986\n",
      " 43.56716679 49.00624607 53.09611882 57.56940353 53.74604937 59.28571429\n",
      " 56.3030185  50.05538476 54.902928   54.53663103 59.28571429 54.93307676\n",
      " 54.57142857 51.79278598 56.7        53.59376924 58.91306033 58.67808317\n",
      " 52.32609867 54.57142857 59.42857143 57.644715   44.67677761 53.18434319\n",
      " 59.42857143 56.3902439  58.46350622 48.93030958 53.90432851 58.5\n",
      " 44.18903358 54.2717243  53.70011266 51.98417182 60.48115125 58.49514684\n",
      " 55.59251203 51.         52.71423498 50.27825633 59.28571429 57.78298725\n",
      " 54.57142857 50.08983039 51.06832366 55.89441327 47.41687516 60.\n",
      " 59.89432177 50.41908411 57.53916173 51.2479433  52.92748306 56.62643533\n",
      " 58.01650422 48.31119379 50.29409041 57.55101864 59.42857143 50.78394451\n",
      " 55.5        51.38134001 52.0909527  59.42857143 48.73889495 58.46350622\n",
      " 49.37767439 49.03652956 56.7        48.95120513 59.42857143 58.76403397\n",
      " 49.         52.32861392 59.42857143 52.64085392 60.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59.  51.  50.  60.  51.  58.  56.  57.  56.  63.  57.  53.  41.  50.\n",
      "  0.  56.  61.  52.5 58.  57.  55.  58.  63.  52.5 49.  44.  59.5 56.\n",
      " 50.  57.  55.  52.  56.  49.  57.  60.  48.  52.5 58.  55.  56.  57.\n",
      " 56.  58.  54.  57.  41.5 51.  61.  49.  54.  45.  49.  52.  47.  59.\n",
      " 51.  59.  58.  63.  53.  60.  59.  50.  62.  51.  61.  53.  58.  49.\n",
      " 30.  56.  59.  51.  57.  54.  52.  53.  52.  52.5 40.  55.  61.  53.\n",
      " 62.  55.  46.  61.  39.  57.  47.  45.5 54.  43.  59.  49.  51.  54.\n",
      " 54.  53.  62.   0. ]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((clf.predict(test_x)-test_y)**2)\n",
    "new_ = clf.predict(test_x)-test_y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
